# python-crawler-basic
A beginner-friendly Python repository for web crawling exercises, featuring easy-to-follow scripts for data extraction from various websites.

## Requirements
To run the scripts in this repository, you need to install several Python packages. You can install them using pip:
```
pip install selenium webdriver_manager requests pandas bs4 
```

## Introduction
This repository is designed for those beginning their journey in web crawling using Python. It utilizes popular libraries such as ***Selenium***, ***Requests***, and ***BeautifulSoup (bs4)***. The scripts provided allow users to extract critical information and images from various websites.

## Scripts Overview

1. [591_rent.py](./591_rent/591_rent.py) : Extracts rental property information and images from the 591 rental website. This script is useful for those looking to analyze housing market trends or find rental properties.
2. [1111_job.py](./1111_job/1111_job.py) : Gathers job listing information from the 1111 job portal. Ideal for job market analysis or personal job searches.
3. [line_sticker.py](./line_sticker/line_sticker.py) : Downloads sticker images from the Line Sticker Store. This can be a fun way to collect stickers or analyze sticker trends.
4. [ntu_announce.py](./ntu_announce/ntu_announce.py) : Retrieves announcement information from the National Taiwan University's official website. Great for students or staff to stay updated with university news.

## Packages
### Selenium
Selenium is a powerful tool for automating web browsers. It allows you to imitate user actions on websites, making it perfect for tasks that require interacting with web pages, such as filling out forms or simulating clicks.

### Requests
The Requests library is used for making HTTP requests in Python. It's a simple way to send requests to web servers and handle responses, ideal for fetching data from APIs or simple web pages.

### BeautifulSoup (bs4)
Beautiful Soup is a library for parsing HTML and XML documents. It creates parse trees that are helpful to extract the data easily. It's commonly used alongside Requests for web scraping tasks.



## Notes
This project is continuously being developed. For questions, suggestions, or contributions, please send an email to easonshen0703@gmail.com. I appreciate your input and will respond as promptly as possible.